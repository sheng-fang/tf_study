{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.losses\n",
    "\n",
    "In the supervised learning, we calculate the gradient from the loss function, which usually contains the estimation error and regularization.\n",
    "\n",
    "In keras model, the regularization in loss function is defined in the layer. For example, the oprtion of kernel_regularizer and bias regularizer in Dense layer. The estimation error is defined during the compilation model. The mean squared error is frequently used for regression problem. The binary_crossentropy is used for binary classification problem, the categorical_crossentropy is for multi-classification problme. \n",
    "\n",
    "Sometime, we also need to define specific error function with y_true and y_pred as the input and a scalar number as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, regularizers, constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 4,810\n",
      "Trainable params: 4,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01), \n",
    "                activity_regularizer=regularizers.l1(0.01),\n",
    "                kernel_constraint = constraints.MaxNorm(max_value=2, axis=0))) \n",
    "model.add(layers.Dense(10,\n",
    "        kernel_regularizer=regularizers.l1_l2(0.01,0.01),activation = \"sigmoid\"))\n",
    "model.compile(optimizer = \"rmsprop\",\n",
    "        loss = \"sparse_categorical_crossentropy\", metrics = [\"AUC\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. build-in loss function\n",
    "\n",
    "    1. mean_squared_error: \"mse\"\n",
    "    2. mean_absolute_error: \"mae\"\n",
    "    3. mean_absolute_percentage_error: \"mape\"\n",
    "    4. Huber\n",
    "    5. binary_crossentropy\n",
    "    6. categorical_crossentropy\n",
    "    7. sparse_categorical_crossentropy\n",
    "    8. hinge: SVM\n",
    "    9. kld: measure the simularity of 2 distributions (EM)\n",
    "    10. cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. custom loss function\n",
    "\n",
    "The custom loss function can be realized by a function or a class inherited from tf.keras.losses.Loss. For the second realization, call function should be overrided to define the calculation.\n",
    "\n",
    "We realize the focal loss as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.sum(alpha * tf.pow(1. - pt_1, gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.sum((1-alpha) * tf.pow( pt_0, gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(losses.Loss):\n",
    "\n",
    "    def __init__(self,gamma=2.0,alpha=0.25):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self,y_true,y_pred):\n",
    "\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        loss = -tf.sum(self.alpha * tf.pow(1. - pt_1, self.gamma) * tf.log(1e-07+pt_1)) \\\n",
    "           -tf.sum((1-self.alpha) * tf.pow( pt_0, self.gamma) * tf.log(1. - pt_0 + 1e-07))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
